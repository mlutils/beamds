{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca410dc1-0814-4397-ba4a-ed802493e0b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from src.beam import beam_arguments, Experiment\n",
    "from src.beam import UniversalDataset, UniversalBatchSampler\n",
    "from src.beam import Algorithm, PackedFolds\n",
    "from src.beam import DataTensor, BeamOptimizer, beam_logger\n",
    "\n",
    "from torchvision import transforms\n",
    "import torchvision\n",
    "from ray import tune\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c56fa4ba-30c7-4430-a44c-c8c154bbbed1",
   "metadata": {},
   "source": [
    "# Deep Learning Projects with Beam: a Step-by-Step Guide"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebdfb50b-1e4f-4165-98cd-0d888fd4eb0b",
   "metadata": {},
   "source": [
    "In this tutorial we'll cover the best practices in building a new beam project. We'll go over all the essential steps when building a project from scratch and using both Jupyter and Pycharm as IDE and developement tools.\n",
    "\n",
    "Usually, you'll get the most out of beam if you **build your project with beam** and avoid porting your existing project to beam. Therefore, we hope that after completing this guide, your first choice will always be to start a beam project from scratch. \n",
    "\n",
    "So, let's start."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610ac087-9c3e-4407-920e-26e4300bccbe",
   "metadata": {},
   "source": [
    "For this tutorial we will use the CIFAR10 classification task.\n",
    "\n",
    "Our steps will be:\n",
    "\n",
    "1. Building a Dataset.\n",
    "2. Building a neural-net.\n",
    "3. Writing an iteration loop.\n",
    "4. Building an algorithm based on the iteration loop.\n",
    "5. Optional: building an argument parser.\n",
    "6. Training your algorithm with default parameters.\n",
    "7. Optimizing your hyperparameters with the Study class.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74901e1d-9fe0-45c5-a318-6011a329dab4",
   "metadata": {},
   "source": [
    "## Building a dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a17e7cb7-23fc-47df-9513-9ecdbe46556f",
   "metadata": {},
   "source": [
    "As usual we'll start with exploring the data. This time we can use torchvision to download the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1be9aab9-27c4-4559-818a-ca72b1149e36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "path_to_data = '/localdata/elads/data/datasets/cifar10'\n",
    "dataset = torchvision.datasets.CIFAR10(root=path_to_data, train=True, transform=torchvision.transforms.PILToTensor(), download=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a8c1cf-0d79-4690-8bda-0015ec8d0d25",
   "metadata": {},
   "source": [
    "Here we used the ```torchvision.transforms.PILToTensor``` to transform the PIL image to tensor. Lets take for example the first image and print its associated label and shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b2d6173-56a4-42e3-bb7d-c295cdba6129",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "torch.Size([3, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "img, label = dataset[0]\n",
    "print(label)\n",
    "print(img.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f3f6aba-b9c5-453e-9163-1f4a4ce849f7",
   "metadata": {},
   "source": [
    "Lets plot the image with matplotlib:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b121234e-bcd0-49a5-abf6-6d49ad5c2808",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f937ecb0880>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(img.permute(1, 2, 0).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b50723eb-a062-4e23-8e79-16af9b2514b2",
   "metadata": {},
   "source": [
    "Now we are ready to build our very first beam object which is the Dataset. We start by building a prototype in the notebook and later we will save it into the project .py file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc8af282-ffe9-4bad-8dde-fc4afd6ef155",
   "metadata": {},
   "source": [
    "First, instead of downloading and preprocessing the data each time, we will store a tensor version of both the train and the test parts of the data. If the file exists, we will directly read the data from the disk. if not, we download it and store the downloaded data as torch pickled object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c88562c-42b2-4459-9bf1-846ef76d5965",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = path_to_data\n",
    "device = torch.device(0)\n",
    "\n",
    "file = os.path.join(path, 'dataset_uint8.pt')\n",
    "if os.path.exists(file):\n",
    "    x_train, x_test, y_train, y_test = torch.load(file, map_location=device)\n",
    "\n",
    "else:\n",
    "    dataset_train = torchvision.datasets.CIFAR10(root=path, train=True,\n",
    "                                                 transform=torchvision.transforms.PILToTensor(), download=True)\n",
    "    dataset_test = torchvision.datasets.CIFAR10(root=path, train=False,\n",
    "                                                transform=torchvision.transforms.PILToTensor(), download=True)\n",
    "\n",
    "    x_train = torch.stack([dataset_train[i][0] for i in range(len(dataset_train))]).to(device)\n",
    "    x_test = torch.stack([dataset_test[i][0] for i in range(len(dataset_test))]).to(device)\n",
    "\n",
    "    y_train = torch.LongTensor(dataset_train.targets).to(device)\n",
    "    y_test = torch.LongTensor(dataset_test.targets).to(device)\n",
    "\n",
    "    torch.save((x_train, x_test, y_train, y_test), file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cb349fe7-24ec-437d-8f5c-e8f147746bf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50000, 3, 32, 32]) torch.Size([10000, 3, 32, 32]) torch.Size([50000]) torch.Size([10000])\n",
      "cuda:0 cuda:0 cuda:0 cuda:0\n",
      "torch.uint8 torch.uint8 torch.int64 torch.int64\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape, x_test.shape, y_train.shape, y_test.shape)\n",
    "print(x_train.device, x_test.device, y_train.device, y_test.device)\n",
    "print(x_train.dtype, x_test.dtype, y_train.dtype, y_test.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d98425-5be4-4ec0-85c8-e237c5f7d217",
   "metadata": {},
   "source": [
    "The Beam Dataset object, works best with a single one-dimentional index to fetch elements from the dataset. However, in our case, we have two objects, the train and the test part. A simple solution could be to concatenate them together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b1a9e53e-3a3d-4428-8d91-d29d942f0ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.cat([x_train, x_test])\n",
    "labels = torch.cat([y_train, y_test])\n",
    "test_indices = len(x_train) + torch.arange(len(x_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f12ecd4f-cd70-4230-9ceb-f1594ac63e32",
   "metadata": {},
   "source": [
    "In this example, we also calculated the test_indices which we would like to store in order to fetch test examples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b0f9334-0cda-4c0e-b674-4303cb348604",
   "metadata": {},
   "source": [
    "While this solution is plausable, Beam is even more fun as it contains a PackedFolds object which is able to hold and index together multiple objects which may have different size. We can use the PackedFolds object to hold the two dataset parts, and calculate the test indices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "fb2c57ca-1e47-4cf7-b1f5-dc5c96116c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = PackedFolds({'train': x_train, 'test': x_test})\n",
    "labels = PackedFolds({'train': y_train, 'test': y_test})\n",
    "test_indices = labels['test'].index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "339930e2-f401-4dba-8be8-a963a5634b66",
   "metadata": {},
   "source": [
    "we can use the PackedFolds object to access each part of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "979b8fbe-4724-4832-993c-936caeb4ceaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50000, 3, 32, 32])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['train'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db949007-1de6-46c5-9706-aa2fdfc77762",
   "metadata": {},
   "source": [
    "We can fetch and slice the data with its indices as a normal tensor. Reffer to the PackedFolds tutorial for further explenations and examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "88f09cb5-eebe-421f-a109-2290418ed1c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': tensor([6, 9, 9,  ..., 9, 1, 1]), 'test': tensor([3, 8, 8,  ..., 5, 1, 7])}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3a6015e7-440a-4670-967b-c6c634eec1f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3, 8, 8,  ..., 5, 1, 7])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b530eca5-b8a5-4449-bc12-dd18f58ccfcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 8, 5,  ..., 5, 1, 7])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[40000:60000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4be0dd-c3e6-4a46-ae56-86839ba65e3c",
   "metadata": {},
   "source": [
    "Lets build a first version of our dataset, without augmentations. We are required to write the ```__getitem__``` method that samples batches from the data for a given index vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "4ba2736e-5a43-4acc-982e-d85fd689204d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CIFAR10Dataset(UniversalDataset):\n",
    "\n",
    "    def __init__(self, path, device):\n",
    "        super().__init__()\n",
    "\n",
    "        file = os.path.join(path, 'dataset_uint8.pt')\n",
    "        if os.path.exists(file):\n",
    "            x_train, x_test, y_train, y_test = torch.load(file, map_location=device)\n",
    "\n",
    "        else:\n",
    "            dataset_train = torchvision.datasets.CIFAR10(root=path, train=True,\n",
    "                                                         transform=torchvision.transforms.PILToTensor(), download=True)\n",
    "            dataset_test = torchvision.datasets.CIFAR10(root=path, train=False,\n",
    "                                                        transform=torchvision.transforms.PILToTensor(), download=True)\n",
    "\n",
    "            x_train = torch.stack([dataset_train[i][0] for i in range(len(dataset_train))]).to(device)\n",
    "            x_test = torch.stack([dataset_test[i][0] for i in range(len(dataset_test))]).to(device)\n",
    "\n",
    "            y_train = torch.LongTensor(dataset_train.targets).to(device)\n",
    "            y_test = torch.LongTensor(dataset_test.targets).to(device)\n",
    "\n",
    "            torch.save((x_train, x_test, y_train, y_test), file)\n",
    "\n",
    "\n",
    "        self.data = PackedFolds({'train': x_train, 'test': x_test})\n",
    "        self.labels = PackedFolds({'train': y_train, 'test': y_test})\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        x = self.data[index]\n",
    "\n",
    "        return {'x': x, 'y': self.labels[index]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "411d313a-d069-4ad4-9df7-012a5ead183f",
   "metadata": {},
   "source": [
    "Lets make sure that we can sample a batch from this dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "183b616c-70c8-4f74-9c8b-a93fa2f7ee19",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CIFAR10Dataset(path_to_data, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "5188d9bb-6742-4c04-ac32-27d9ceb372c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'x': torch.Size([10, 3, 32, 32]), 'y': torch.Size([10])}\n"
     ]
    }
   ],
   "source": [
    "d = dataset[10:20]\n",
    "print({k: v.shape for k, v in d.items()})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c38933c-a79e-41fe-b97d-c54220c64447",
   "metadata": {},
   "source": [
    "while the objects ```dataset.data``` and ```dataset.labels``` contain two folds, i.e. train and test, the dataset still is not aware of the different folds and their meaning. In addition, we usually would like to split the train subset into a train-validation sets. For the dataset split purposes, beam datasets have the split method which is able to split/random split/time based split your data. It also requires a seed to generate reproducable splits. In our case, to allot 20% of the examples to the validation subset, we will call the split method as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b83299d1-5687-4ae2-b309-c84ba9dcae93",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 5782\n",
    "\n",
    "dataset.split(validation=.2, test=dataset.labels['test'].index, seed=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f4e7f81-d4e2-44fa-9a58-821d4d821ba0",
   "metadata": {},
   "source": [
    "Now the dataset contains the ```indices_split``` dictionary which holds all the different subsets. They will be used to further generate samplers and dataloaders to iterate over the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "3095959f-0a9e-4e14-ae58-8a947be6e64e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'test': 10000, 'validation': 12000, 'train': 38000}\n"
     ]
    }
   ],
   "source": [
    "print({k: len(v) for k, v in dataset.indices_split.items()})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c421239-939d-48ff-a22c-a1246b3f71ba",
   "metadata": {},
   "source": [
    "In addition we can define different augmentations for the train/eval parts of the training. A nice property of the Beam dataset when combined with Beam algorithm is that like neural networks, it holds the ```training``` boolean property which sets the dataset in train/eval mode. The algorithm iterator toggles this property between train and eval sessions by calling ```dataset.train()``` or ```dataset.eval()``` (as with ```nn.Module``` objects). We can use this property to define different s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d19c7e48-5b5f-4e14-b70e-b202cecf0a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "padding = 4\n",
    "\n",
    "augmentations = transforms.Compose([transforms.RandomHorizontalFlip(),\n",
    "                                    transforms.RandomCrop(32, padding=padding, padding_mode='edge'),])\n",
    "\n",
    "self.t_basic = transforms.Compose([transforms.Lambda(lambda x: (x / 255)),\n",
    "                                    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))])\n",
    "\n",
    "self.t_train = transforms.Compose([augmentations, self.t_basic])\n",
    "\n",
    "file = os.path.join(path, 'dataset_uint8.pt')\n",
    "if os.path.exists(file):\n",
    "    x_train, x_test, y_train, y_test = torch.load(file, map_location=device)\n",
    "\n",
    "else:\n",
    "    dataset_train = torchvision.datasets.CIFAR10(root=path, train=True,\n",
    "                                                 transform=torchvision.transforms.PILToTensor(), download=True)\n",
    "    dataset_test = torchvision.datasets.CIFAR10(root=path, train=False,\n",
    "                                                transform=torchvision.transforms.PILToTensor(), download=True)\n",
    "\n",
    "    x_train = torch.stack([dataset_train[i][0] for i in range(len(dataset_train))]).to(device)\n",
    "    x_test = torch.stack([dataset_test[i][0] for i in range(len(dataset_test))]).to(device)\n",
    "\n",
    "    y_train = torch.LongTensor(dataset_train.targets).to(device)\n",
    "    y_test = torch.LongTensor(dataset_test.targets).to(device)\n",
    "\n",
    "    torch.save((x_train, x_test, y_train, y_test), file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ca410dc1-0814-4397-ba4a-ed802493e0b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from src.beam import beam_arguments, Experiment, get_beam_parser\n",
    "from src.beam import UniversalDataset, UniversalBatchSampler\n",
    "from src.beam import Algorithm, PackedFolds\n",
    "from src.beam import DataTensor, BeamOptimizer, beam_logger\n",
    "\n",
    "from torchvision import transforms\n",
    "import torchvision\n",
    "from ray import tune\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c56fa4ba-30c7-4430-a44c-c8c154bbbed1",
   "metadata": {},
   "source": [
    "# Deep Learning Projects with Beam: a Step-by-Step Guide"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebdfb50b-1e4f-4165-98cd-0d888fd4eb0b",
   "metadata": {},
   "source": [
    "In this tutorial we'll cover the best practices in building a new beam project. We'll go over all the essential steps when building a project from scratch and using both Jupyter and Pycharm as IDE and developement tools.\n",
    "\n",
    "Usually, you'll get the most out of beam if you **build your project with beam** and avoid porting your existing project to beam. Therefore, we hope that after completing this guide, your first choice will always be to start a beam project from scratch. \n",
    "\n",
    "So, let's start."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610ac087-9c3e-4407-920e-26e4300bccbe",
   "metadata": {},
   "source": [
    "For this tutorial we will use the CIFAR10 classification task.\n",
    "\n",
    "Our steps will be:\n",
    "\n",
    "1. Setting up an experiment. Optional: building an argument parser.\n",
    "2. Building a Dataset.\n",
    "3. Building a neural-net.\n",
    "4. Writing an iteration loop.\n",
    "5. Building an algorithm based on the iteration loop.\n",
    "6. Training your algorithm with default parameters.\n",
    "7. Optimizing your hyperparameters with the Study class.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f343ade-02b7-4a50-b794-5063c30be8c5",
   "metadata": {},
   "source": [
    "## Setting up an experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd6574f-fadf-4acb-8349-781cefd43439",
   "metadata": {},
   "source": [
    "Before setting up the experiment, lets define two directories: ```path_to_data``` which will contain the dataset information and ```root_dir``` which will contain the project results and log files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2e861685-54cb-4585-8cfc-f8cb78b7e2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path_to_data = '/localdata/elads/data/datasets/cifar10'\n",
    "# root_dir = '/localdata/elads/data/cifar10'\n",
    "\n",
    "path_to_data = '/home/shared/data/dataset/cifar10'\n",
    "root_dir = '/home/shared/data/results/cifar10'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a65907a-c82f-4ab4-b063-428b78024995",
   "metadata": {},
   "source": [
    "The beam experiment class object, deals with all the trifles, i.e., parts in the training process which are not directly related to algorithm. This includes:   \n",
    "1. Storing the Hyperparameters.\n",
    "2. Constructing the experimeny logging directory and storing the code, the trained model and a log file.\n",
    "3. Printing results to the screen, storing them into the disk and sending them to a tensorboard writer.\n",
    "4. managing parallel runs.\n",
    "\n",
    "This design choice helps in maintaining a clean Algorithm class which is not linked to any particular experiment or even ×© dataset and can be later directly shiped for deployment without any code refactoring."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f94828-1483-49d7-9ec1-f8cb9ed17c45",
   "metadata": {},
   "source": [
    "Our very first beam object would be the experiment hyperparameters. Beam has a set of default hyperparamters and the user may extra hyperparamters on top. If we work in a jupyter notebook, we call ```beam_arguments``` to generate the hyperparameter object. When working natively with python it is best to set the hyperparameters as script arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4853b7a2-ebc4-4e38-a294-f00e7c60eb1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = beam_arguments()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0f82e0-358c-4c32-be0e-10810275de77",
   "metadata": {},
   "source": [
    "User can set existing hyperparameters by passing them as strings or named arguments to the ```beam_arguments``` function. You can also dynamically define new arguments. In the following example we added a few parameters such as ```channels``` and ```activation``` which we are going to use later in our algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d269e406-3192-4f15-a221-55c1f10c68b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = beam_arguments(f\"--project-name=cifar10 --root-dir=/tmp/cifar --algorithm=CIFAR10Algorithm --device=0 --amp --lr-d=1e-2 --batch-size=512\",\n",
    "                      \"--n-epochs=40 --clip-gradient=1000 --parallel=0 --accumulate=1 --no-deterministic\",\n",
    "                      \"--weight-decay=3e-5 --beta1=0.9\", \n",
    "                      path_to_data=path_to_data, dropout=.0, activation='relu', channels=1024, label_smoothing=.2,\n",
    "                      padding=6, gain=.2, turn_point=512, final_point=4096, minimal_gain=.05, temperature=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8b5a03db-a72c-4995-88db-9473a23eec50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Namespace(accumulate=1, activation='relu', algorithm='CIFAR10Algorithm', amp=True, batch_size=512, batch_size_eval=None, batch_size_train=None, beta1=0.9, beta2=0.999, channels=1024, clip_gradient=1000.0, cpu_workers=0, deterministic=False, device='0', dropout=0.0, enable_tqdm=True, epoch_length=None, epoch_length_eval=None, epoch_length_train=None, eps=0.0001, expansion_size=10000000, final_point=4096, gain=0.2, half=False, identifier='debug', init='ortho', label_smoothing=0.2, lognet=True, lr_dense=0.01, lr_sparse=0.01, minimal_gain=0.05, mp_port='random', n_epochs=40, override=False, oversample=False, oversampling_weight_factor=1.0, padding=6, parallel=0, path_to_data='/home/shared/data/dataset/cifar10', print_results=True, project_name='cifar10', reload=False, resume=-1, root_dir='/tmp/cifar', scale_epoch_by_batch_size=True, seed=0, split_dataset_seed=5782, store_initial_weights=False, store_networks='logscale', store_results='logscale', temperature=0.05, tensorboard=True, total_steps=1000000, turn_point=512, visualize_results='yes', visualize_weights=True, weight_decay=3e-05)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hparams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa789bf-6c94-45d3-a8ec-2dcdb046aaa8",
   "metadata": {},
   "source": [
    "Based on the generated hyperparameters, we can set up a new experiment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8285f7bc-d73b-4c33-b7e7-25fb14a3b621",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2022-07-11 13:50:33\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mCreating new experiment\u001b[0m\n",
      "\u001b[32m2022-07-11 13:50:33\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExperiment directory is: /tmp/cifar/cifar10/CIFAR10Algorithm/debug/0003_20220711_135033\u001b[0m\n",
      "\u001b[32m2022-07-11 13:50:33\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mbeam project: cifar10\u001b[0m\n",
      "\u001b[32m2022-07-11 13:50:33\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExperiment Hyperparameters\u001b[0m\n",
      "\u001b[32m2022-07-11 13:50:33\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mproject_name: cifar10\u001b[0m\n",
      "\u001b[32m2022-07-11 13:50:33\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1malgorithm: CIFAR10Algorithm\u001b[0m\n",
      "\u001b[32m2022-07-11 13:50:33\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1midentifier: debug\u001b[0m\n",
      "\u001b[32m2022-07-11 13:50:33\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mmp_port: random\u001b[0m\n",
      "\u001b[32m2022-07-11 13:50:33\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mroot_dir: /tmp/cifar\u001b[0m\n",
      "\u001b[32m2022-07-11 13:50:33\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mreload: False\u001b[0m\n",
      "\u001b[32m2022-07-11 13:50:33\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mresume: -1\u001b[0m\n",
      "\u001b[32m2022-07-11 13:50:33\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1moverride: False\u001b[0m\n",
      "\u001b[32m2022-07-11 13:50:33\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mcpu_workers: 0\u001b[0m\n",
      "\u001b[32m2022-07-11 13:50:33\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mdevice: 0\u001b[0m\n",
      "\u001b[32m2022-07-11 13:50:33\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mparallel: 0\u001b[0m\n",
      "\u001b[32m2022-07-11 13:50:33\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mtensorboard: True\u001b[0m\n",
      "\u001b[32m2022-07-11 13:50:33\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mlognet: True\u001b[0m\n",
      "\u001b[32m2022-07-11 13:50:33\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mdeterministic: False\u001b[0m\n",
      "\u001b[32m2022-07-11 13:50:33\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mscale_epoch_by_batch_size: True\u001b[0m\n",
      "\u001b[32m2022-07-11 13:50:33\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mhalf: False\u001b[0m\n",
      "\u001b[32m2022-07-11 13:50:33\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mamp: True\u001b[0m\n",
      "\u001b[32m2022-07-11 13:50:33\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mstore_initial_weights: False\u001b[0m\n",
      "\u001b[32m2022-07-11 13:50:33\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1moversample: False\u001b[0m\n",
      "\u001b[32m2022-07-11 13:50:33\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1minit: ortho\u001b[0m\n",
      "\u001b[32m2022-07-11 13:50:33\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mseed: 0\u001b[0m\n",
      "\u001b[32m2022-07-11 13:50:33\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1msplit_dataset_seed: 5782\u001b[0m\n",
      "\u001b[32m2022-07-11 13:50:33\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mtotal_steps: 1000000\u001b[0m\n",
      "\u001b[32m2022-07-11 13:50:33\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mepoch_length: None\u001b[0m\n",
      "\u001b[32m2022-07-11 13:50:33\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mepoch_length_train: None\u001b[0m\n",
      "\u001b[32m2022-07-11 13:50:33\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mepoch_length_eval: None\u001b[0m\n",
      "\u001b[32m2022-07-11 13:50:33\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mn_epochs: 40\u001b[0m\n",
      "\u001b[32m2022-07-11 13:50:33\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mbatch_size: 512\u001b[0m\n",
      "\u001b[32m2022-07-11 13:50:33\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mbatch_size_train: None\u001b[0m\n",
      "\u001b[32m2022-07-11 13:50:33\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mbatch_size_eval: None\u001b[0m\n",
      "\u001b[32m2022-07-11 13:50:33\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mlr_dense: 0.01\u001b[0m\n",
      "\u001b[32m2022-07-11 13:50:33\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mlr_sparse: 0.01\u001b[0m\n",
      "\u001b[32m2022-07-11 13:50:33\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mweight_decay: 3e-05\u001b[0m\n",
      "\u001b[32m2022-07-11 13:50:33\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1meps: 0.0001\u001b[0m\n",
      "\u001b[32m2022-07-11 13:50:33\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mbeta1: 0.9\u001b[0m\n",
      "\u001b[32m2022-07-11 13:50:33\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mbeta2: 0.999\u001b[0m\n",
      "\u001b[32m2022-07-11 13:50:33\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mclip_gradient: 1000.0\u001b[0m\n",
      "\u001b[32m2022-07-11 13:50:33\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1maccumulate: 1\u001b[0m\n",
      "\u001b[32m2022-07-11 13:50:33\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1moversampling_weight_factor: 1.0\u001b[0m\n",
      "\u001b[32m2022-07-11 13:50:33\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mexpansion_size: 10000000\u001b[0m\n",
      "\u001b[32m2022-07-11 13:50:33\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mprint_results: True\u001b[0m\n",
      "\u001b[32m2022-07-11 13:50:33\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mvisualize_weights: True\u001b[0m\n",
      "\u001b[32m2022-07-11 13:50:33\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1menable_tqdm: True\u001b[0m\n",
      "\u001b[32m2022-07-11 13:50:33\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mvisualize_results: yes\u001b[0m\n",
      "\u001b[32m2022-07-11 13:50:33\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mstore_results: logscale\u001b[0m\n",
      "\u001b[32m2022-07-11 13:50:33\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mstore_networks: logscale\u001b[0m\n",
      "\u001b[32m2022-07-11 13:50:33\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mpath_to_data: /home/shared/data/dataset/cifar10\u001b[0m\n",
      "\u001b[32m2022-07-11 13:50:33\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mdropout: 0.0\u001b[0m\n",
      "\u001b[32m2022-07-11 13:50:33\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mactivation: relu\u001b[0m\n",
      "\u001b[32m2022-07-11 13:50:33\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mchannels: 1024\u001b[0m\n",
      "\u001b[32m2022-07-11 13:50:33\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mlabel_smoothing: 0.2\u001b[0m\n",
      "\u001b[32m2022-07-11 13:50:33\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mpadding: 6\u001b[0m\n",
      "\u001b[32m2022-07-11 13:50:33\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mgain: 0.2\u001b[0m\n",
      "\u001b[32m2022-07-11 13:50:33\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mturn_point: 512\u001b[0m\n",
      "\u001b[32m2022-07-11 13:50:33\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mfinal_point: 4096\u001b[0m\n",
      "\u001b[32m2022-07-11 13:50:33\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mminimal_gain: 0.05\u001b[0m\n",
      "\u001b[32m2022-07-11 13:50:33\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mtemperature: 0.05\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "experiment = Experiment(hparams)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e83f03c-03ed-41b5-b5cf-6c33f132efef",
   "metadata": {},
   "source": [
    "While we can dynamically add as much parameters as we wish, it is best if we set default values (and optionally help strings) to each parameter that we define in our experiment. For this purpose we need to define a new parser on top of the default parser and call ```beam_arguments``` with the first argument as the new parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3cbbe9c0-a2cd-47ab-a268-be8006139ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cifar10_parser():\n",
    "\n",
    "    parser = get_beam_parser()\n",
    "    parser.add_argument('--path-to-data', type=str, default='/home/shared/data/dataset/cifar10', help='Where to store the dataset')\n",
    "    parser.add_argument('--dropout', type=float, default=0.0, help='Dropout value for the feature layer')\n",
    "    parser.add_argument('--activation', type=str, default='relu', help='Type of activation function. supported types: [relu, celu, gelu]')\n",
    "    parser.add_argument('--channels', type=int, default=1024, help='Final channel size in the convolution stack')\n",
    "    parser.add_argument('--label-smoothing', type=float, default=0.2, help='a label_smoothing factor for the CrossEntropy loss')\n",
    "    parser.add_argument('--temperature', type=float, default=0.05, help='a softmax temperature scaling')\n",
    "    parser.add_argument('--padding', type=int, default=6, help='Padding number for the crop augmentation')\n",
    "    parser.add_argument('--gain', type=float, default=0.2, help='a gain factor for the learning rate scheduler')\n",
    "    parser.add_argument('--turn-point', type=int, default=512, help='a turn_point factor for the learning rate scheduler')\n",
    "    parser.add_argument('--final-point', type=int, default=4096, help='a final_point factor for the learning rate scheduler')\n",
    "    parser.add_argument('--minimal-gain', type=float, default=0.05, help='a minimal_gain factor for the learning rate scheduler')\n",
    "\n",
    "    return parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "720f3680-6c28-48a8-b29b-b49015a1a02a",
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = beam_arguments(get_cifar10_parser(), f\"--project-name=cifar10 --root-dir=/tmp/cifar --algorithm=CIFAR10Algorithm --device=0 --amp --lr-d=1e-2 --batch-size=512\",\n",
    "                      \"--n-epochs=40 --clip-gradient=1000 --parallel=0 --accumulate=1 --no-deterministic\",\n",
    "                      \"--weight-decay=3e-5 --beta1=0.9\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74901e1d-9fe0-45c5-a318-6011a329dab4",
   "metadata": {},
   "source": [
    "## Building a dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a17e7cb7-23fc-47df-9513-9ecdbe46556f",
   "metadata": {},
   "source": [
    "As usual we'll start with exploring the data. This time we can use torchvision to download the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1be9aab9-27c4-4559-818a-ca72b1149e36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "path_to_data = '/localdata/elads/data/datasets/cifar10'\n",
    "dataset = torchvision.datasets.CIFAR10(root=path_to_data, train=True, transform=torchvision.transforms.PILToTensor(), download=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a8c1cf-0d79-4690-8bda-0015ec8d0d25",
   "metadata": {},
   "source": [
    "Here we used the ```torchvision.transforms.PILToTensor``` to transform the PIL image to tensor. Lets take for example the first image and print its associated label and shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "9b2d6173-56a4-42e3-bb7d-c295cdba6129",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "torch.Size([3, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "img, label = dataset[0]\n",
    "print(label)\n",
    "print(img.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f3f6aba-b9c5-453e-9163-1f4a4ce849f7",
   "metadata": {},
   "source": [
    "Lets plot the image with matplotlib:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "b121234e-bcd0-49a5-abf6-6d49ad5c2808",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f1691db4640>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(img.permute(1, 2, 0).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b50723eb-a062-4e23-8e79-16af9b2514b2",
   "metadata": {},
   "source": [
    "Now we are ready to build our very first beam object which is the Dataset. We start by building a prototype in the notebook and later we will save it into the project .py file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc8af282-ffe9-4bad-8dde-fc4afd6ef155",
   "metadata": {},
   "source": [
    "First, instead of downloading and preprocessing the data each time, we will store a tensor version of both the train and the test parts of the data. If the file exists, we will directly read the data from the disk. if not, we download it and store the downloaded data as torch pickled object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "1c88562c-42b2-4459-9bf1-846ef76d5965",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = path_to_data\n",
    "device = torch.device(0)\n",
    "\n",
    "file = os.path.join(path, 'dataset_uint8.pt')\n",
    "if os.path.exists(file):\n",
    "    x_train, x_test, y_train, y_test = torch.load(file, map_location=device)\n",
    "\n",
    "else:\n",
    "    dataset_train = torchvision.datasets.CIFAR10(root=path, train=True,\n",
    "                                                 transform=torchvision.transforms.PILToTensor(), download=True)\n",
    "    dataset_test = torchvision.datasets.CIFAR10(root=path, train=False,\n",
    "                                                transform=torchvision.transforms.PILToTensor(), download=True)\n",
    "\n",
    "    x_train = torch.stack([dataset_train[i][0] for i in range(len(dataset_train))]).to(device)\n",
    "    x_test = torch.stack([dataset_test[i][0] for i in range(len(dataset_test))]).to(device)\n",
    "\n",
    "    y_train = torch.LongTensor(dataset_train.targets).to(device)\n",
    "    y_test = torch.LongTensor(dataset_test.targets).to(device)\n",
    "\n",
    "    torch.save((x_train, x_test, y_train, y_test), file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "cb349fe7-24ec-437d-8f5c-e8f147746bf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50000, 3, 32, 32]) torch.Size([10000, 3, 32, 32]) torch.Size([50000]) torch.Size([10000])\n",
      "cuda:0 cuda:0 cuda:0 cuda:0\n",
      "torch.uint8 torch.uint8 torch.int64 torch.int64\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape, x_test.shape, y_train.shape, y_test.shape)\n",
    "print(x_train.device, x_test.device, y_train.device, y_test.device)\n",
    "print(x_train.dtype, x_test.dtype, y_train.dtype, y_test.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d98425-5be4-4ec0-85c8-e237c5f7d217",
   "metadata": {},
   "source": [
    "The Beam Dataset object, works best with a single one-dimentional index to fetch elements from the dataset. However, in our case, we have two objects, the train and the test part. A simple solution could be to concatenate them together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b1a9e53e-3a3d-4428-8d91-d29d942f0ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.cat([x_train, x_test])\n",
    "labels = torch.cat([y_train, y_test])\n",
    "test_indices = len(x_train) + torch.arange(len(x_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f12ecd4f-cd70-4230-9ceb-f1594ac63e32",
   "metadata": {},
   "source": [
    "In this example, we also calculated the test_indices which we would like to store in order to fetch test examples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b0f9334-0cda-4c0e-b674-4303cb348604",
   "metadata": {},
   "source": [
    "While this solution is plausable, Beam is even more fun as it contains a PackedFolds object which is able to hold and index together multiple objects which may have different size. We can use the PackedFolds object to hold the two dataset parts, and calculate the test indices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "fb2c57ca-1e47-4cf7-b1f5-dc5c96116c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = PackedFolds({'train': x_train, 'test': x_test})\n",
    "labels = PackedFolds({'train': y_train, 'test': y_test})\n",
    "test_indices = labels['test'].index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "339930e2-f401-4dba-8be8-a963a5634b66",
   "metadata": {},
   "source": [
    "we can use the PackedFolds object to access each part of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "979b8fbe-4724-4832-993c-936caeb4ceaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50000, 3, 32, 32])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['train'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db949007-1de6-46c5-9706-aa2fdfc77762",
   "metadata": {},
   "source": [
    "We can fetch and slice the data with its indices as a normal tensor. Reffer to the PackedFolds tutorial for further explenations and examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "88f09cb5-eebe-421f-a109-2290418ed1c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': tensor([6, 9, 9,  ..., 9, 1, 1]), 'test': tensor([3, 8, 8,  ..., 5, 1, 7])}"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "3a6015e7-440a-4670-967b-c6c634eec1f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3, 8, 8,  ..., 5, 1, 7])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b530eca5-b8a5-4449-bc12-dd18f58ccfcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 8, 5,  ..., 5, 1, 7])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[40000:60000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4be0dd-c3e6-4a46-ae56-86839ba65e3c",
   "metadata": {},
   "source": [
    "Lets build a first version of our dataset, without augmentations. We are required to write the ```__getitem__``` method that samples batches from the data for a given index vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "4ba2736e-5a43-4acc-982e-d85fd689204d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CIFAR10Dataset(UniversalDataset):\n",
    "\n",
    "    def __init__(self, hparams):\n",
    "        \n",
    "        path = hparams.path_to_data\n",
    "        device = hparams.device\n",
    "        \n",
    "        super().__init__()\n",
    "\n",
    "        file = os.path.join(path, 'dataset_uint8.pt')\n",
    "        if os.path.exists(file):\n",
    "            x_train, x_test, y_train, y_test = torch.load(file, map_location=device)\n",
    "\n",
    "        else:\n",
    "            dataset_train = torchvision.datasets.CIFAR10(root=path, train=True,\n",
    "                                                         transform=torchvision.transforms.PILToTensor(), download=True)\n",
    "            dataset_test = torchvision.datasets.CIFAR10(root=path, train=False,\n",
    "                                                        transform=torchvision.transforms.PILToTensor(), download=True)\n",
    "\n",
    "            x_train = torch.stack([dataset_train[i][0] for i in range(len(dataset_train))]).to(device)\n",
    "            x_test = torch.stack([dataset_test[i][0] for i in range(len(dataset_test))]).to(device)\n",
    "\n",
    "            y_train = torch.LongTensor(dataset_train.targets).to(device)\n",
    "            y_test = torch.LongTensor(dataset_test.targets).to(device)\n",
    "\n",
    "            torch.save((x_train, x_test, y_train, y_test), file)\n",
    "\n",
    "\n",
    "        self.data = PackedFolds({'train': x_train, 'test': x_test})\n",
    "        self.labels = PackedFolds({'train': y_train, 'test': y_test})\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        x = self.data[index]\n",
    "\n",
    "        return {'x': x, 'y': self.labels[index]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "411d313a-d069-4ad4-9df7-012a5ead183f",
   "metadata": {},
   "source": [
    "Notice that we passed ```hparams``` as an argument to the Dataset constructor. This will be our standard ```__init__``` format when working with beam, i.e. we pass all the arguments for the dataset in the ```hparams``` container. Now, lets make sure that we can sample a batch from this dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "183b616c-70c8-4f74-9c8b-a93fa2f7ee19",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CIFAR10Dataset(path_to_data, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "5188d9bb-6742-4c04-ac32-27d9ceb372c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'x': torch.Size([10, 3, 32, 32]), 'y': torch.Size([10])}\n"
     ]
    }
   ],
   "source": [
    "d = dataset[10:20]\n",
    "print({k: v.shape for k, v in d.items()})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c38933c-a79e-41fe-b97d-c54220c64447",
   "metadata": {},
   "source": [
    "while the objects ```dataset.data``` and ```dataset.labels``` contain two folds, i.e. train and test, the dataset still is not aware of the different folds and their meaning. In addition, we usually would like to split the train subset into a train-validation sets. For the dataset split purposes, beam datasets have the split method which is able to split/random split/time based split your data. It also requires a seed to generate reproducable splits. In our case, to allot 20% of the examples to the validation subset, we will call the split method as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "b83299d1-5687-4ae2-b309-c84ba9dcae93",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 5782\n",
    "\n",
    "dataset.split(validation=.2, test=dataset.labels['test'].index, seed=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f4e7f81-d4e2-44fa-9a58-821d4d821ba0",
   "metadata": {},
   "source": [
    "Now the dataset contains the ```indices_split``` dictionary which holds all the different subsets. They will be used to further generate samplers and dataloaders to iterate over the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "3095959f-0a9e-4e14-ae58-8a947be6e64e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'test': 10000, 'validation': 12000, 'train': 38000}\n"
     ]
    }
   ],
   "source": [
    "print({k: len(v) for k, v in dataset.indices_split.items()})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c421239-939d-48ff-a22c-a1246b3f71ba",
   "metadata": {},
   "source": [
    "In addition we can define different augmentations for the train/eval parts of the training. A nice property of the Beam dataset when combined with Beam algorithm is that like neural networks, it holds the ```training``` boolean property which sets the dataset in train/eval mode. The algorithm iterator toggles this property between train and eval sessions by calling ```dataset.train()``` or ```dataset.eval()``` (as with ```nn.Module``` objects). We can use this property to define different augmentations for each case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "0a261186-6cea-4055-b001-e843f15a932d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CIFAR10Dataset(UniversalDataset):\n",
    "\n",
    "    def __init__(self, hparams, use_folds=True):\n",
    "        super().__init__()\n",
    "\n",
    "        path = hparams.path_to_data\n",
    "        device = hparams.device\n",
    "        padding = hparams.padding\n",
    "\n",
    "        augmentations = transforms.Compose([transforms.RandomHorizontalFlip(),\n",
    "                                            transforms.RandomCrop(32, padding=padding, padding_mode='edge'),])\n",
    "\n",
    "        self.t_basic = transforms.Compose([transforms.Lambda(lambda x: (x / 255)),\n",
    "                                            transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))])\n",
    "\n",
    "        self.t_train = transforms.Compose([augmentations, self.t_basic])\n",
    "\n",
    "        file = os.path.join(path, 'dataset_uint8.pt')\n",
    "        if os.path.exists(file):\n",
    "            x_train, x_test, y_train, y_test = torch.load(file, map_location=device)\n",
    "\n",
    "        else:\n",
    "            dataset_train = torchvision.datasets.CIFAR10(root=path, train=True,\n",
    "                                                         transform=torchvision.transforms.PILToTensor(), download=True)\n",
    "            dataset_test = torchvision.datasets.CIFAR10(root=path, train=False,\n",
    "                                                        transform=torchvision.transforms.PILToTensor(), download=True)\n",
    "\n",
    "            x_train = torch.stack([dataset_train[i][0] for i in range(len(dataset_train))]).to(device)\n",
    "            x_test = torch.stack([dataset_test[i][0] for i in range(len(dataset_test))]).to(device)\n",
    "\n",
    "            y_train = torch.LongTensor(dataset_train.targets).to(device)\n",
    "            y_test = torch.LongTensor(dataset_test.targets).to(device)\n",
    "\n",
    "            torch.save((x_train, x_test, y_train, y_test), file)\n",
    "\n",
    "        if use_folds:\n",
    "            self.data = PackedFolds({'train': x_train, 'test': x_test})\n",
    "            self.labels = PackedFolds({'train': y_train, 'test': y_test})\n",
    "            self.split(validation=.2, test=self.labels['test'].index, seed=experiment.args.split_dataset_seed)\n",
    "        \n",
    "        else:\n",
    "            self.data = torch.cat([x_train, x_test])\n",
    "            self.labels = torch.cat([y_train, y_test])\n",
    "            test_indices = len(x_train) + torch.arange(len(x_test))\n",
    "            self.split(validation=.2, test=test_indices, seed=hparams.split_dataset_seed)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        x = self.data[index]\n",
    "\n",
    "        if self.training:\n",
    "            x = self.t_train(x)\n",
    "        else:\n",
    "            x = self.t_basic(x)\n",
    "\n",
    "        x = x.to(memory_format=torch.channels_last)\n",
    "\n",
    "        return {'x': x, 'y': self.labels[index]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "557b3b0c-0bfe-458e-bb33-3a850bc01481",
   "metadata": {},
   "source": [
    "Notice, that we also added the ```hparams.device``` argument which defines the device for storing ```torch.Tensor``` objects in this experiemnt. Therefore, we map the data to the designated device. Finally, we added an option to work with simple tensors instead of ```PackedFolds```."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcfcd001-a4a7-45b0-a8a8-1bfd03336e00",
   "metadata": {},
   "source": [
    "It is important to measure the fetching speed from the dataset. The most straitforward way to obtain that would be with ```tqdm```:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "013b1c7c-812d-463a-86c8-70aa101b570d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: '[0]'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [102]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mValueError\u001b[0m: could not convert string to float: '[0]'"
     ]
    }
   ],
   "source": [
    "try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "42e442c5-a348-4f66-8c90-7cbc1d85c9d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.device"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(torch.device(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "ef331555-5766-4652-8d71-ccd2f10ef339",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "don't know how to restore data location of torch._UntypedStorage (tagged with 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [87]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mCIFAR10Dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_folds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [86]\u001b[0m, in \u001b[0;36mCIFAR10Dataset.__init__\u001b[0;34m(self, hparams, use_folds)\u001b[0m\n\u001b[1;32m     18\u001b[0m file \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdataset_uint8.pt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(file):\n\u001b[0;32m---> 20\u001b[0m     x_train, x_test, y_train, y_test \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     23\u001b[0m     dataset_train \u001b[38;5;241m=\u001b[39m torchvision\u001b[38;5;241m.\u001b[39mdatasets\u001b[38;5;241m.\u001b[39mCIFAR10(root\u001b[38;5;241m=\u001b[39mpath, train\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     24\u001b[0m                                                  transform\u001b[38;5;241m=\u001b[39mtorchvision\u001b[38;5;241m.\u001b[39mtransforms\u001b[38;5;241m.\u001b[39mPILToTensor(), download\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/serialization.py:712\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    710\u001b[0m             opened_file\u001b[38;5;241m.\u001b[39mseek(orig_position)\n\u001b[1;32m    711\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mload(opened_file)\n\u001b[0;32m--> 712\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpickle_load_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    713\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _legacy_load(opened_file, map_location, pickle_module, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/serialization.py:1046\u001b[0m, in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1044\u001b[0m unpickler \u001b[38;5;241m=\u001b[39m UnpicklerWrapper(data_file, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args)\n\u001b[1;32m   1045\u001b[0m unpickler\u001b[38;5;241m.\u001b[39mpersistent_load \u001b[38;5;241m=\u001b[39m persistent_load\n\u001b[0;32m-> 1046\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43munpickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1048\u001b[0m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_validate_loaded_sparse_tensors()\n\u001b[1;32m   1050\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/serialization.py:1016\u001b[0m, in \u001b[0;36m_load.<locals>.persistent_load\u001b[0;34m(saved_id)\u001b[0m\n\u001b[1;32m   1014\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m loaded_storages:\n\u001b[1;32m   1015\u001b[0m     nbytes \u001b[38;5;241m=\u001b[39m numel \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_element_size(dtype)\n\u001b[0;32m-> 1016\u001b[0m     \u001b[43mload_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_maybe_decode_ascii\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1018\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loaded_storages[key]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/serialization.py:1001\u001b[0m, in \u001b[0;36m_load.<locals>.load_tensor\u001b[0;34m(dtype, numel, key, location)\u001b[0m\n\u001b[1;32m    997\u001b[0m storage \u001b[38;5;241m=\u001b[39m zip_file\u001b[38;5;241m.\u001b[39mget_storage_from_record(name, numel, torch\u001b[38;5;241m.\u001b[39m_UntypedStorage)\u001b[38;5;241m.\u001b[39mstorage()\u001b[38;5;241m.\u001b[39m_untyped()\n\u001b[1;32m    998\u001b[0m \u001b[38;5;66;03m# TODO: Once we decide to break serialization FC, we can\u001b[39;00m\n\u001b[1;32m    999\u001b[0m \u001b[38;5;66;03m# stop wrapping with _TypedStorage\u001b[39;00m\n\u001b[1;32m   1000\u001b[0m loaded_storages[key] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstorage\u001b[38;5;241m.\u001b[39m_TypedStorage(\n\u001b[0;32m-> 1001\u001b[0m     wrap_storage\u001b[38;5;241m=\u001b[39m\u001b[43mrestore_location\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m   1002\u001b[0m     dtype\u001b[38;5;241m=\u001b[39mdtype)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/serialization.py:970\u001b[0m, in \u001b[0;36m_get_restore_location.<locals>.restore_location\u001b[0;34m(storage, location)\u001b[0m\n\u001b[1;32m    969\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrestore_location\u001b[39m(storage, location):\n\u001b[0;32m--> 970\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdefault_restore_location\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/serialization.py:179\u001b[0m, in \u001b[0;36mdefault_restore_location\u001b[0;34m(storage, location)\u001b[0m\n\u001b[1;32m    177\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    178\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[0;32m--> 179\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdon\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt know how to restore data location of \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    180\u001b[0m                    \u001b[38;5;241m+\u001b[39m torch\u001b[38;5;241m.\u001b[39mtypename(storage) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m (tagged with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    181\u001b[0m                    \u001b[38;5;241m+\u001b[39m location \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: don't know how to restore data location of torch._UntypedStorage (tagged with 0)"
     ]
    }
   ],
   "source": [
    "dataset = CIFAR10Dataset(hparams, use_folds=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

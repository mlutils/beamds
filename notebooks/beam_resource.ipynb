{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e79b4a8-f4c0-4cf3-9d41-c8c90b9170d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up the Beam environment for interactive use\n",
      "Standard modules will be automatically imported so you can use them without explicit import\n",
      "Done importing packages. It took:  3.9 seconds\n",
      "Beam library is loaded from path: /home/dsi/elads/projects/beamds/notebooks/../src/beam\n",
      "The Beam version is: 2.3.2\n"
     ]
    }
   ],
   "source": [
    "%load_ext beam_setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02235cb7-0887-4e4b-9da9-8aaef2f9eb95",
   "metadata": {},
   "source": [
    "# Use resource to access files and folders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "464d81dd-7fc3-4397-9cd7-d23b41dbe915",
   "metadata": {},
   "source": [
    "A resource can be a file or folder path on: (1) local filesystem, s3 and s3 compatible filesystem, hdfs and sftp connection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1460cba8-550d-4371-b05a-5588182ecc11",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = resource('/tmp/path/to/folder')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe37106-f71f-4f66-a74c-ff96cd5160d1",
   "metadata": {},
   "source": [
    "The path resource follows the pathlib api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "036508ef-073e-4ba4-b7d3-a0805e2e4573",
   "metadata": {},
   "outputs": [],
   "source": [
    "path.mkdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a5407742-e955-48f5-9d2c-a2be1963e34a",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = path.joinpath('a.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d97785ac-80b5-44cf-886f-350f0f9ec5cf",
   "metadata": {},
   "source": [
    "but it also supports read and write operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "888fd578-6e87-47b7-afe1-9927ddb13861",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "file:///tmp/path/to/folder/a.pt"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path.write(torch.randn(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "36c6b06c-d014-4741-b16c-163e3a3895ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1.1119, -1.1612,  1.7486,  2.4973])\n"
     ]
    }
   ],
   "source": [
    "print(file_path.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c648c4-b2a8-44cd-aa7b-3bff56ab5111",
   "metadata": {},
   "source": [
    "it supports many file types including: torch (.pt), pickle (.pkl), feather (.fea), parquet (.parquet) and many more (see beam.path.core.PureBeamPath read and write operations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "196ac82a-2b8a-4078-9d28-eae972e0753e",
   "metadata": {},
   "source": [
    "we can also specify how we would like to store the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "60ae459b-4538-4221-a103-82f9de8a6675",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "file:///tmp/path/to/folder/some_name_with_no_extention"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path.joinpath('some_name_with_no_extention').write(np.random.randn(4), ext='.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e5669375-50c9-44d3-916a-9d6f590ec9a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.12440085, -0.25151748, -0.99421076,  0.03682782])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path.joinpath('some_name_with_no_extention').read(ext='.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8895445-a25b-42ff-8154-6930f62612bb",
   "metadata": {},
   "source": [
    "we can also iter and list folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a1ee70ca-2307-40c7-9201-492906a44a59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[file:///tmp/path/to/folder/a.pt,\n",
       " file:///tmp/path/to/folder/some_name_with_no_extention]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(path.iterdir())\n",
    "list(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf8373a-f3c4-4efe-ac92-ec13040cb5b8",
   "metadata": {},
   "source": [
    "To access paths on different storage platforms use the standard URI conventions:\n",
    "\n",
    "path = resource('scheme://\\<hostname\\>:\\<port\\>/path/to/my/file?arg1=val1&arg2=val2')\n",
    "\n",
    "Some examples:\n",
    "* s3 on AWS: s3:///\\<bucket name\\>/<\\object\\>?access_key=\\<my aws access key\\>&secret_key=\\<my secret access key\\>\n",
    "* s3 on Minio: s3://\\<hostname\\>:\\<port\\>/\\<bucket name\\>/<\\object\\>?access_key=\\<my aws access key\\>&secret_key=\\<my secret access key\\>?tls=false\n",
    "* HDFS: hdfs://\\<hostname\\>:\\<http connection port usually 9870\\>/path/to/my/file?access_key=\\<my hdfs access key\\>?tls=\\<whether connecting via https\\>\n",
    "\n",
    "\n",
    "Note that you can replace the scheme s3 with s3-pa and hdfs with hdfs-pa to get access via pyarrow which can increase performance instead of native implementations like boto3.\n",
    "For hdfs-pa you may need to replace the port to the data node communication port: usually at 50010 (consult hdfs-site.xml and core-site.xml files for details)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca253ad-79c8-4f5d-93d6-67859cebd36e",
   "metadata": {},
   "source": [
    "# Use resource to access Large Language Models (LLMs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71921e7c-ca18-4971-abe5-e97dd6109d3d",
   "metadata": {},
   "source": [
    "You can use resource also to access LLMs on various platforms: openai, fastchat, tgi, internal fastapi, local huggingface"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db19b17d-7f35-4b11-bb31-cd4bccef8778",
   "metadata": {},
   "source": [
    "before accessing to the LLM, note that we can store access keys to our environment and to a local file s.t. it stays permanenty in our system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "6a55bd40-d314-4529-bf82-9f4d83b87dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this would print the stored key in your system\n",
    "# beam_key['OPENAI_API_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "948a7099-4d09-46e1-ae97-26377cbe9f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this will assign a new key to your system\n",
    "# beam_key['OPENAI_API_KEY'] = <my new key>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4fc4bea3-4c15-4646-a0b0-b967ede02ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = resource('openai:///gpt-4')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a44a816-df83-4cac-81e3-6db91cf98239",
   "metadata": {},
   "source": [
    "you can give any model instructions or chat with it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f134e4a6-0cca-4b7d-9f26-69ff45dee0e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Israel was founded on May 14, 1948.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.ask('when israel was founded? give me exact date').text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b14c60cd-388e-480b-b5f3-3390495baf02",
   "metadata": {},
   "source": [
    "you can chat with the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dd392e50-1797-4fbe-a99b-229ddd32d4df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello Elad! How can I assist you today?'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.chat('Hi my name is elad').text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8619dd6a-0bb4-4347-bc2b-46c6729b289b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Yes, your name is Elad. How can I assist you further?'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.chat('Hi again, do you remember my name?').text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "baee08f4-ec4b-46c0-a207-6b27e7e7f3e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"As an AI, I don't have the ability to remember personal data unless it's shared in the current conversation. I am designed to respect user privacy and confidentiality.\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.chat('Hi again, do you remember my name?', reset_chat=True).text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4dc2886-36eb-40c4-a98f-64d23df5a1c8",
   "metadata": {},
   "source": [
    "You can also parse the response in other formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fddd02ce-d88e-451a-86b6-150fcc36cbcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': \"I'm an AI and don't have feelings, but I'm functioning as expected\",\n",
       " 'message': 'Ready to assist you'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.ask('Hi how are you today? answer in a JSON format').json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "87f75131-2bbd-4122-9f88-47ae5aad1d55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': {'message': \"I'm an AI, I don't have feelings, but I'm functioning as expected.\",\n",
       "  'code': 200}}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.ask('Hi how are you today? answer in a YAML format').yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dadc302d-7cb4-42d6-82bd-5eae9d1ec6da",
   "metadata": {},
   "source": [
    "You can use the LLM also directly with langchain without any further wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c3433203-a3eb-4b10-8535-f1693e18478a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import HumanMessage\n",
    "\n",
    "text = \"What would be a good company name for a company that makes colorful socks?\"\n",
    "messages = [HumanMessage(content=text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c735a938-c02b-47f9-a632-2c53a7f3be30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1. \"Rainbow Steps\"\\n2. \"ColorStride\"\\n3. \"Vibrant Toes\"\\n4. \"Spectrum Socks\"\\n5. \"Kaleidoscope Kicks\"\\n6. \"Technicolor Treads\"\\n7. \"Prismatic Peds\"\\n8. \"Colorful Comfort\"\\n9. \"Spectrum Steps\"\\n10. \"Vivid Footprints\"\\n11. \"ColorPop Socks\"\\n12. \"Rainbow Walk\"\\n13. \"Vibrant Walk\"\\n14. \"Hue Crew\"\\n15. \"Colorful Cozies\"\\n16. \"Bright Steps\"\\n17. \"Kaleidosocks\"\\n18. \"Colorful Cushions\"\\n19. \"Spectrum Soles\"\\n20. \"Rainbow Wraps\"'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ee4df864-2f17-4422-a871-a9026e3a5ee4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"Rainbow Steps\"'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d5847a-405f-4f38-9eea-7429db8f8011",
   "metadata": {},
   "source": [
    "With our URIs, You can also use openai syntax with any model (not just openai), simply import our simulator instead of openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "611de8d2-19e0-4d56-92c6-15766c71506b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from beam.llm import openai_simulator as openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0719af4-00db-4a97-9aa7-5a8b2993a7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = resource('openai:///gpt-4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a96fc28f-451e-4247-9e43-1122a72f4633",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<OpenAIObject text_completion id=cmpl-8LUaFSIYElAhWp8lHG6doTZPl8AFA at 0x7f2424166ca0> JSON: {\n",
       "  \"warning\": \"This model version is deprecated. Migrate before January 4, 2024 to avoid disruption of service. Learn more https://platform.openai.com/docs/deprecations\",\n",
       "  \"id\": \"cmpl-8LUaFSIYElAhWp8lHG6doTZPl8AFA\",\n",
       "  \"object\": \"text_completion\",\n",
       "  \"created\": 1700133143,\n",
       "  \"model\": \"text-davinci-003\",\n",
       "  \"choices\": [\n",
       "    {\n",
       "      \"text\": \"\\n\\n1024\",\n",
       "      \"index\": 0,\n",
       "      \"logprobs\": null,\n",
       "      \"finish_reason\": \"stop\"\n",
       "    }\n",
       "  ],\n",
       "  \"usage\": {\n",
       "    \"prompt_tokens\": 5,\n",
       "    \"completion_tokens\": 3,\n",
       "    \"total_tokens\": 8\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "openai.Completion.create(prompt='2**10=?', model='openai:///text-davinci-003')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06aef35a-1a58-41b1-b972-bb30094ec0c1",
   "metadata": {},
   "source": [
    "To access other LLM resource types follow the URI convention:\n",
    "\n",
    "llm = resource('scheme://\\<hostname\\>:\\<port\\>/path/to/my/file?arg1=val1&arg2=val2')\n",
    "\n",
    "possible schemes: openai, fastchat, tgi, fastapi, huggingface"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ec2b48-79a9-4087-9981-73ca0c253ab5",
   "metadata": {},
   "source": [
    "# Use resource to access BeamServer algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a525b13-c700-478e-b632-c2fb15875a0b",
   "metadata": {},
   "source": [
    "You can use beam also to quickly deploy an algorithm via ssh. You can then access this algorithm with a resource object from any machine that can access the server."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29700130-bab8-4f97-a3cb-c7e15092778d",
   "metadata": {},
   "source": [
    "For example, here we build a sparse similarity server (like faiss but for sparse vectors e.g. TFIDF vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "567508ce-4b29-40e2-8ace-f9b0c5dae7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from beam.sparse import SparseSimilarity\n",
    "from beam.serve import BeamServer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a685a2da-716c-431e-9651-9fef4d9382da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-11-16 12:06:56\u001b[0m | BeamLog | \u001b[1mINFO\u001b[0m | \u001b[1mOpening a flask inference serve on port: 28951\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "M = 40000\n",
    "sparse_sim = SparseSimilarity(metric='cosine', format='coo', vec_size=M, device='cuda', k=10)\n",
    "server = BeamServer(sparse_sim)\n",
    "server.run_non_blocking(server='waitress')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2652e85a-18a0-4809-8070-d2d102048b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_coo_vectors(k, M, nel):\n",
    "\n",
    "    r = []\n",
    "    c = []\n",
    "    v = []\n",
    "\n",
    "    for i in range(k):\n",
    "        r.append(i * torch.ones(nel, dtype=torch.int64))\n",
    "        c.append(torch.randint(M, size=(nel,)))\n",
    "        v.append(torch.randn(nel))\n",
    "\n",
    "    return torch.sparse_coo_tensor(torch.stack([torch.cat(r), torch.cat(c)]), torch.cat(v), size=(k, M))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6365775a-69cc-42b4-8f29-d2037a34294f",
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = gen_coo_vectors(20000, M, 100)\n",
    "s2 = gen_coo_vectors(20, M, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d7b12794-9718-416b-9b74-7802b4eabe0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_sim = resource('beam-server://localhost:28951')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fd027dca-5d72-4dcc-a136-954b92f84753",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_sim.add(s1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a253b8f0-32ac-463b-9348-eb15ed8e65d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.58 s, sys: 274 ms, total: 1.86 s\n",
      "Wall time: 229 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dist, ind = sparse_sim.search(s2, k=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a23abf2-8c09-4a39-9ab5-09ad1c824b1d",
   "metadata": {},
   "source": [
    "The beam server can wrap any function or class for quick and easy deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f3def476-5db9-41ac-9f21-a86162f2d849",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_executor_and_store(prompt, llm='openai:///text-davinci-003?max_tokens=2048'):\n",
    "    llm = resource(llm)\n",
    "    code = llm.ask(f\"Return executable python code that performs the following task. The final result should assigned to a variable name 'res':\\n{prompt}\\n\\n\\n\").text\n",
    "    try:\n",
    "        exec(code)\n",
    "        return res, code\n",
    "    except:\n",
    "        return 'ExecutionError', code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c01a15ea-fea7-4aea-9b70-b7333eb0c8aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-11-16 12:25:20\u001b[0m | BeamLog | \u001b[1mINFO\u001b[0m | \u001b[1mOpening a flask inference serve on port: 28952\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "server = BeamServer(llm_executor_and_store)\n",
    "server.run_non_blocking(server='waitress')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5af21a34-0f9d-49ee-822d-d89615da26b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "remote_executor = resource('beam-server://localhost:28952')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "61107d02-9615-4921-86ef-05a414c5ad21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def fibonacci(n): \n",
      "    a = 0\n",
      "    b = 1\n",
      "    if n < 0: \n",
      "        print(\"Incorrect input\") \n",
      "    elif n == 0: \n",
      "        return a \n",
      "    elif n == 1: \n",
      "        return b \n",
      "    else: \n",
      "        for i in range(2,n): \n",
      "            c = a + b \n",
      "            a = b \n",
      "            b = c \n",
      "        return b \n",
      "  \n",
      "res = fibonacci(18)\n",
      "1597\n"
     ]
    }
   ],
   "source": [
    "r, c = remote_executor('what is the 18th number in the fibonacci series?')\n",
    "\n",
    "print(c)\n",
    "print(r)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

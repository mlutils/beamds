{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2faf82e-014f-4a46-867d-bd1de529aba8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up the Beam environment for interactive use\n",
      "Standard modules will be automatically imported so you can use them without explicit import\n",
      "Done importing packages. It took:  3.8 seconds\n",
      "Beam library is loaded from path: /home/mlspeech/elads/projects/beamds/src/beam\n",
      "The Beam version is: 2.4.8b\n"
     ]
    }
   ],
   "source": [
    "%load_ext beam_setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9ac0205-a9a8-49de-b91f-2a843b5e40b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from beam.assistant import BeamAssistant"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a2d1c0-3f34-41cb-977d-936fd7b346f9",
   "metadata": {},
   "source": [
    "## natural language interaction with beam_server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "34ebb95b-db2e-4728-bceb-1043bdab64df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from beam.serve import beam_server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92997f46-7475-403d-80bc-3f19407b0a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def func(x):\n",
    "    return sorted(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "722784f3-ec6f-45e1-93e8-c2193eb047a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "assistant = BeamAssistant(beam_server, llm='openai:///gpt-4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c758bfb0-b3e6-4a00-a95a-6f47d1489f12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The function `beam_server` is used to create a server with a specified protocol, host, port, backend, and other parameters. It supports both HTTP and gRPC protocols. If the protocol is HTTP, it checks if 'tls' is in the keyword arguments, if not, it sets 'tls' to True if the protocol is 'https', otherwise False. If a backend is provided, it is set as the server in the run_kwargs dictionary. The function then imports the appropriate server class based on the protocol and creates a server instance. If the `non_blocking` parameter is True, it runs the server in a non-blocking mode, otherwise, it runs the server in a blocking mode. The function finally returns the server instance. If an unknown protocol is provided, it raises a ValueError.\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assistant.summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "014268f2-cc1a-45f2-bca9-33d0d10917c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-03-12 21:22:24\u001b[0m | BeamLog | \u001b[1mINFO\u001b[0m | \u001b[1mUsing args: ['<user_arg>my_func</user_arg>'] and kwargs: {'protocol': 'https', 'port': 23455, 'non_blocking': True} to answer the query\u001b[0m\n",
      "\u001b[32m2024-03-12 21:22:24\u001b[0m | BeamLog | \u001b[1mINFO\u001b[0m | \u001b[1mOpening a flask inference serve on port: 23455\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<beam.serve.http_server.HTTPServer at 0x7fe199200430>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assistant.exec('serve my function with https on port 23455, do not block the process', my_func=func)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e98553-f4b4-4287-bd11-f53ffd8e6f00",
   "metadata": {},
   "source": [
    "## the assistant can also use eval to evaluate expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5241df4e-1d62-4fba-ae3e-5707ff8648e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def func(v: torch.tensor, sig: float, mu: float):\n",
    "    return sig * v + mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "970016be-f0b7-482f-ac29-4a99295dddb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "assistant = BeamAssistant(func, llm='openai:///gpt-4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "62deb0af-3a75-46c8-b134-bb031c0bb03f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-03-12 21:23:39\u001b[0m | BeamLog | \u001b[1mINFO\u001b[0m | \u001b[1mUsing args: ['<eval>torch.randn(10)</eval>', 2, 0.5] and kwargs: {} to answer the query\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 1.0070,  0.1916,  0.2589, -1.1061,  0.8634,  2.9715,  3.2597,  0.0457,\n",
       "         2.7627,  2.8694])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assistant.exec('generate 10 samples from a gaussian distribution with parameters N(0.5, 2)')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

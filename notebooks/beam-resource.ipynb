{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d82aa42-3ba6-4d4e-8d26-702d68debb8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up the Beam environment for interactive use\n",
      "Standard modules will be automatically imported so you can use them without explicit import\n",
      "Done importing packages. It took:  4.4 seconds\n",
      "Beam library is loaded from path: /home/elad/docker/beamds/beam\n",
      "The Beam version is: 2.5.12\n"
     ]
    }
   ],
   "source": [
    "%load_ext beam_setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad0762f-3ac4-493d-87da-0f0dbbbb1222",
   "metadata": {},
   "source": [
    "# Beam Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83e0996-e7e1-486d-8af2-1e62a913ae3b",
   "metadata": {},
   "source": [
    "Beam features a unified and simplified *resource* API. A resource is defined as an **external entity** that operates outside the immediate memory space or execution context of the current program. Common examples include files, remote storage services, webpages, and remote algorithms. Conveniently, each resource can be uniquely identified by a [URI](https://en.wikipedia.org/wiki/Uniform_Resource_Identifier) (Uniform Resource Identifier). This URI is a string that specifies the resource type (scheme), its address, and additional metadata required to interact with it effectively.\n",
    "\n",
    "As a programmer, when developing software, your primary concern often isn't the internal mechanics of a resource but rather the capabilities it offers through its API. For instance, in the context of storage, you might be interested in how you can read from and write to files efficiently, regardless of their physical location. Indeed, it would be beneficial if you could switch seamlessly between resources as needed. For example, if you initially used NFS for file storage and later decided to switch to S3, you’d prefer to make this transition with minimal changes to your program.\n",
    "\n",
    "Beam simplifies this process beautifully. It allows you to transition between different resources merely by updating a URI string in your configuration files. This means you can switch resources without any code changes, significantly easing development and maintenance.\n",
    "\n",
    "Here are the resources which Beam currently supports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f4c8954-4827-424e-8eb4-ba44ae143fbc",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "path=['file', 's3', 's3-pa', 'hdfs', 'hdfs-pa', 'sftp', 'comet', 'io', 'dict', 'redis', 'smb', 'nt', 'mlflow'],\n",
      "serve=['beam-http', 'beam-https', 'beam-grpc', 'triton', 'triton-http', 'triton-grpc', 'triton-https', 'triton-grpcs'],\n",
      "distributed=['async-http', 'async-https'],\n",
      "llm=['openai', 'vllm', 'tgi', 'fastchat', 'huggingface', 'samurai', 'samur-openai', 'fastapi-dp'],\n",
      "triton=['triton', 'triton-http', 'triton-grpc', 'triton-https', 'triton-grpcs'],\n",
      "ray=['ray']\n"
     ]
    }
   ],
   "source": [
    "from beam.resources import resource_names\n",
    "from beam.utils import pprint, pretty_print_dict\n",
    "print(pretty_print_dict(resource_names, dense=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ce7d11-9163-4d4d-82d6-297452f52215",
   "metadata": {},
   "source": [
    "As you can see Beam supports many different storage endpoints. Imprtantly, they all share the same API, i.e. a [pathlib](https://docs.python.org/3/library/pathlib.html) augmented API that facilitates the interaction with files and folders and make sure that you can easily switch between different endpointsץ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02235cb7-0887-4e4b-9da9-8aaef2f9eb95",
   "metadata": {},
   "source": [
    "## Path Like Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "464d81dd-7fc3-4397-9cd7-d23b41dbe915",
   "metadata": {},
   "source": [
    "All resources are accessed by a single function named *resouce*, by providing their URI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "642ad028-bd2e-4dba-9355-71b1a10633fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from beam import resource"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192bf79f-3156-4ef3-b123-fa3be12c8829",
   "metadata": {},
   "source": [
    "Lets start with interacting with the simplest resource, i.e. a file, in this case we do not need to provide the scheme or the location (file is the default scheme)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1460cba8-550d-4371-b05a-5588182ecc11",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = resource('/tmp/path/to/folder')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe37106-f71f-4f66-a74c-ff96cd5160d1",
   "metadata": {},
   "source": [
    "The path resource follows the pathlib api, so functions like: mkdir, joinpath, exists are implemented:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "036508ef-073e-4ba4-b7d3-a0805e2e4573",
   "metadata": {},
   "outputs": [],
   "source": [
    "path.mkdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "00bad13b-44a9-4318-87d9-bfe0e4d98574",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a5407742-e955-48f5-9d2c-a2be1963e34a",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = path.joinpath('a.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4395fe3f-882b-4de9-b8cd-374eda154401",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path.exists()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d97785ac-80b5-44cf-886f-350f0f9ec5cf",
   "metadata": {},
   "source": [
    "but it also supports read and write operations which determine the file type via the extention string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "888fd578-6e87-47b7-afe1-9927ddb13861",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "file:///tmp/path/to/folder/a.pt"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path.write(torch.randn(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "36c6b06c-d014-4741-b16c-163e3a3895ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1.1139,  0.1157, -1.0144, -0.8021])\n"
     ]
    }
   ],
   "source": [
    "print(file_path.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c648c4-b2a8-44cd-aa7b-3bff56ab5111",
   "metadata": {},
   "source": [
    "it supports many file types including: torch (.pt), pickle (.pkl), feather (.fea), parquet (.parquet) and many more (see beam.path.core.PureBeamPath read and write operations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "196ac82a-2b8a-4078-9d28-eae972e0753e",
   "metadata": {},
   "source": [
    "we can also specify how we would like to store the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "60ae459b-4538-4221-a103-82f9de8a6675",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "file:///tmp/path/to/folder/some_name_with_no_extention"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path.joinpath('some_name_with_no_extention').write(np.random.randn(4), ext='.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e5669375-50c9-44d3-916a-9d6f590ec9a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.06095397, -0.21831003,  1.29672265,  0.57001603])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path.joinpath('some_name_with_no_extention').read(ext='.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8895445-a25b-42ff-8154-6930f62612bb",
   "metadata": {},
   "source": [
    "we can also iter and list folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a1ee70ca-2307-40c7-9201-492906a44a59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[file:///tmp/path/to/folder/some_name_with_no_extention,\n",
       " file:///tmp/path/to/folder/a.pt]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(path.iterdir())\n",
    "list(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf8373a-f3c4-4efe-ac92-ec13040cb5b8",
   "metadata": {},
   "source": [
    "To access paths on different storage platforms use the standard URI conventions:\n",
    "\n",
    "path = resource('scheme://\\<hostname\\>:\\<port\\>/path/to/my/file?arg1=val1&arg2=val2')\n",
    "\n",
    "Some examples:\n",
    "* s3 on AWS: s3:///\\<bucket name\\>/<\\object\\>?access_key=\\<my aws access key\\>&secret_key=\\<my secret access key\\>\n",
    "* s3 on Minio: s3://\\<hostname\\>:\\<port\\>/\\<bucket name\\>/<\\object\\>?access_key=\\<my aws access key\\>&secret_key=\\<my secret access key\\>?tls=false\n",
    "* HDFS: hdfs://\\<hostname\\>:\\<http connection port usually 9870\\>/path/to/my/file?access_key=\\<my hdfs access key\\>?tls=\\<whether connecting via https\\>\n",
    "\n",
    "\n",
    "Note that you can replace the scheme s3 with s3-pa and hdfs with hdfs-pa to get access via pyarrow which can increase performance instead of native implementations like boto3.\n",
    "For hdfs-pa you may need to replace the port to the data node communication port: usually at 50010 (consult hdfs-site.xml and core-site.xml files for details)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d3e372-7f23-43b7-a446-2470ec69a757",
   "metadata": {},
   "source": [
    "For example, here we connect to a MinIO S3 compatible storage and repeat the same API commands we used when connecting to local files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9e0fd59-9255-4611-8f9a-5caa19e5c47e",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = resource('s3://172.17.0.1:9000/sandbox/?access-key=myaccesskey&secret-key=mysecretkey&tls=false')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab9e2f5a-4d18-4f03-8e1a-958d012cde6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = path.joinpath('a.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "82ac88b1-8080-4e42-97c4-8cfc041dd615",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "s3://172.17.0.1:9000/sandbox/a.pt"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path.write(torch.randn(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8e218d34-3b04-4bd3-9c09-f55ff8f01012",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.3245,  1.0341,  0.9378, -2.1612])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca253ad-79c8-4f5d-93d6-67859cebd36e",
   "metadata": {},
   "source": [
    "## Use resource to access Large Language Models (LLMs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71921e7c-ca18-4971-abe5-e97dd6109d3d",
   "metadata": {},
   "source": [
    "You can use resource also to access LLMs on various platforms: openai, fastchat, tgi, internal fastapi, local huggingface"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db19b17d-7f35-4b11-bb31-cd4bccef8778",
   "metadata": {},
   "source": [
    "before accessing to the LLM, note that we can store access keys to our environment and to a local file s.t. it stays permanenty in our system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6a55bd40-d314-4529-bf82-9f4d83b87dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this would print the stored key in your system\n",
    "# beam_key['OPENAI_API_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "948a7099-4d09-46e1-ae97-26377cbe9f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this will assign a new key to your system\n",
    "# beam_key['OPENAI_API_KEY'] = 'my_key_here'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4fc4bea3-4c15-4646-a0b0-b967ede02ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = resource('openai:///gpt-4')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a44a816-df83-4cac-81e3-6db91cf98239",
   "metadata": {},
   "source": [
    "you can give any model instructions or chat with it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f134e4a6-0cca-4b7d-9f26-69ff45dee0e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Israel was founded on May 14, 1948.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.ask('when israel was founded? give me exact date').text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b14c60cd-388e-480b-b5f3-3390495baf02",
   "metadata": {},
   "source": [
    "you can chat with the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dd392e50-1797-4fbe-a99b-229ddd32d4df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello Elad! How can I assist you today?'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.chat('Hi my name is elad').text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8619dd6a-0bb4-4347-bc2b-46c6729b289b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Yes, your name is Elad. How can I assist you further?'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.chat('Hi again, do you remember my name?').text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "baee08f4-ec4b-46c0-a207-6b27e7e7f3e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"As an AI, I don't have the ability to remember personal data unless it's shared with me in the course of our conversation. I am designed to respect user privacy and confidentiality.\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.chat('Hi again, do you remember my name?', reset_chat=True).text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4dc2886-36eb-40c4-a98f-64d23df5a1c8",
   "metadata": {},
   "source": [
    "You can also parse the response in other formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fddd02ce-d88e-451a-86b6-150fcc36cbcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'success',\n",
       " 'message': \"I'm an AI, I don't have feelings, but I'm functioning as expected.\"}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.ask('Hi how are you today? answer in a JSON format').json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "87f75131-2bbd-4122-9f88-47ae5aad1d55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': {'response': \"I'm an AI, I don't have feelings, but I'm functioning as expected. Thank you for asking.\"}}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.ask('Hi how are you today? answer in a YAML format').yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dadc302d-7cb4-42d6-82bd-5eae9d1ec6da",
   "metadata": {},
   "source": [
    "You can use the LLM also directly with langchain without any further wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c3433203-a3eb-4b10-8535-f1693e18478a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import HumanMessage\n",
    "\n",
    "text = \"What would be a good company name for a company that makes colorful socks?\"\n",
    "messages = [HumanMessage(content=text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c735a938-c02b-47f9-a632-2c53a7f3be30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1. \"Rainbow Steps\"\\n2. \"Colorful Cozies\"\\n3. \"Spectrum Socks\"\\n4. \"Vibrant Footprints\"\\n5. \"Kaleidoscope Kicks\"\\n6. \"Technicolor Toes\"\\n7. \"Prismatic Peds\"\\n8. \"ColorSocks\"\\n9. \"Vivid Steps\"\\n10. \"Hue Crew Socks\"\\n11. \"ColorStride\"\\n12. \"BrightSox\"\\n13. \"Spectrum Strides\"\\n14. \"Colorful Comfort\"\\n15. \"Rainbow Wraps\"\\n16. \"Vivid Veils\"\\n17. \"ChromaSox\"\\n18. \"Colorful Cushions\"\\n19. \"Kaleidosocks\"\\n20. \"Rainbow Rugs\".'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ee4df864-2f17-4422-a871-a9026e3a5ee4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"Rainbow Steps\"'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d5847a-405f-4f38-9eea-7429db8f8011",
   "metadata": {},
   "source": [
    "With our URIs, You can also use openai syntax with any model (not just openai), simply import our simulator instead of openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "611de8d2-19e0-4d56-92c6-15766c71506b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from beam.llm import openai_simulator as openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c0719af4-00db-4a97-9aa7-5a8b2993a7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = resource('openai:///gpt-4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a96fc28f-451e-4247-9e43-1122a72f4633",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<OpenAIObject text_completion id=cmpl-8V35kGo6b7yXlv2Vsmgo6yaNLqEos at 0x7f6c5118fe70> JSON: {\n",
       "  \"warning\": \"This model version is deprecated. Migrate before January 4, 2024 to avoid disruption of service. Learn more https://platform.openai.com/docs/deprecations\",\n",
       "  \"id\": \"cmpl-8V35kGo6b7yXlv2Vsmgo6yaNLqEos\",\n",
       "  \"object\": \"text_completion\",\n",
       "  \"created\": 1702410744,\n",
       "  \"model\": \"text-davinci-003\",\n",
       "  \"choices\": [\n",
       "    {\n",
       "      \"text\": \"\\n\\n1024\",\n",
       "      \"index\": 0,\n",
       "      \"logprobs\": null,\n",
       "      \"finish_reason\": \"stop\"\n",
       "    }\n",
       "  ],\n",
       "  \"usage\": {\n",
       "    \"prompt_tokens\": 5,\n",
       "    \"completion_tokens\": 3,\n",
       "    \"total_tokens\": 8\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "openai.Completion.create(prompt='2**10=?', model='openai:///text-davinci-003')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06aef35a-1a58-41b1-b972-bb30094ec0c1",
   "metadata": {},
   "source": [
    "To access other LLM resource types follow the URI convention:\n",
    "\n",
    "llm = resource('scheme://\\<hostname\\>:\\<port\\>/path/to/my/file?arg1=val1&arg2=val2')\n",
    "\n",
    "possible schemes: openai, fastchat, tgi, fastapi, huggingface"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ec2b48-79a9-4087-9981-73ca0c253ab5",
   "metadata": {},
   "source": [
    "## Use resource to access BeamServer algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a525b13-c700-478e-b632-c2fb15875a0b",
   "metadata": {},
   "source": [
    "You can use beam also to quickly deploy an algorithm via ssh. You can then access this algorithm with a resource object from any machine that can access the server."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29700130-bab8-4f97-a3cb-c7e15092778d",
   "metadata": {},
   "source": [
    "For example, here we build a sparse similarity server (like faiss but for sparse vectors e.g. TFIDF vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "567508ce-4b29-40e2-8ace-f9b0c5dae7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from beam.sparse import SparseSimilarity\n",
    "from beam.serve import beam_server\n",
    "M = 40000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a685a2da-716c-431e-9651-9fef4d9382da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-12-12 19:56:59\u001b[0m | BeamLog | \u001b[1mINFO\u001b[0m | \u001b[1mOpening a flask inference serve on port: 27450\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "sparse_sim = SparseSimilarity(metric='cosine', format='coo', vec_size=M, device='cpu', k=10)\n",
    "server = beam_server(sparse_sim, backend='waitress', non_blocking=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2652e85a-18a0-4809-8070-d2d102048b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_coo_vectors(k, M, nel):\n",
    "\n",
    "    r = []\n",
    "    c = []\n",
    "    v = []\n",
    "\n",
    "    for i in range(k):\n",
    "        r.append(i * torch.ones(nel, dtype=torch.int64))\n",
    "        c.append(torch.randint(M, size=(nel,)))\n",
    "        v.append(torch.randn(nel))\n",
    "\n",
    "    return torch.sparse_coo_tensor(torch.stack([torch.cat(r), torch.cat(c)]), torch.cat(v), size=(k, M))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6365775a-69cc-42b4-8f29-d2037a34294f",
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = gen_coo_vectors(20000, M, 100)\n",
    "s2 = gen_coo_vectors(20, M, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d7b12794-9718-416b-9b74-7802b4eabe0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_sim = resource('beam-http://localhost:27450')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ef1bfab3-7c6c-4548-b9ed-fc49b5893755",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_sim.add(s1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a253b8f0-32ac-463b-9348-eb15ed8e65d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UserWarning: Sparse CSR tensor support is in beta state. If you miss a functionality in the sparse tensor support, please submit a feature request to https://github.com/pytorch/pytorch/issues. (Triggered internally at /opt/pytorch/pytorch/aten/src/ATen/SparseCsrTensorImpl.cpp:53.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.44 s, sys: 1.59 s, total: 4.03 s\n",
      "Wall time: 532 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dist, ind = sparse_sim.search(s2, k=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a23abf2-8c09-4a39-9ab5-09ad1c824b1d",
   "metadata": {},
   "source": [
    "The beam server can wrap any function or class for quick and easy deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f3def476-5db9-41ac-9f21-a86162f2d849",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_executor_and_store(prompt, llm='openai:///text-davinci-003?max_tokens=2048'):\n",
    "    llm = resource(llm)\n",
    "    code = llm.ask(f\"Return executable python code that performs the following task. The final result should assigned to a variable name 'res':\\n{prompt}\\n\\n\\n\").text\n",
    "    try:\n",
    "        exec(code)\n",
    "        return res, code\n",
    "    except:\n",
    "        return 'ExecutionError', code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c01a15ea-fea7-4aea-9b70-b7333eb0c8aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<beam.serve.http_server.HTTPServer at 0x7f6c506df5e0>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-12-12 19:58:09\u001b[0m | BeamLog | \u001b[1mINFO\u001b[0m | \u001b[1mOpening a flask inference serve on port: 27451\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "beam_server(llm_executor_and_store, backend='waitress', non_blocking=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5af21a34-0f9d-49ee-822d-d89615da26b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "remote_executor = resource('beam-http://localhost:27451')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "61107d02-9615-4921-86ef-05a414c5ad21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def fibonacci(n): \n",
      "    a = 0\n",
      "    b = 1\n",
      "    if n < 0: \n",
      "        print(\"Incorrect input\") \n",
      "    elif n == 0: \n",
      "        return a \n",
      "    elif n == 1: \n",
      "        return b \n",
      "    else: \n",
      "        for i in range(2,n): \n",
      "            c = a + b \n",
      "            a = b \n",
      "            b = c \n",
      "        return b \n",
      "  \n",
      "res = fibonacci(18)\n",
      "ExecutionError\n"
     ]
    }
   ],
   "source": [
    "r, c = remote_executor('what is the 18th number in the fibonacci series?')\n",
    "\n",
    "print(c)\n",
    "print(r)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
